\capitulo{5}{Aspectos relevantes del desarrollo del proyecto} \label{capitulo5}

El proyecto se divide en una fase preliminar, consistente en la adecuación del código previo, correspondiente en el TFG
el cual se ha hecho extensible en este trabajo y tres propuestas diferentes para la identificación biomédica de personas 
a través de una imagen de su ojo utilizando el iris como parte central de este reconocimiento.

\section{Optimización de los notebooks previos}

La primera fase trata de la optimización del código previo y la creación de un pipeline con una configuración que permitiese un mejor control de las funciones.
Se han creado funciones correspondientes a cada una de las fases y se han encapsulado en forma de partes de una pipeline, con el único argumento de entrada de la 
configuración. 

\imagen{img/03_workflow}{Ejemplo de workflow del paper \cite{malgheet_iris_2021}}{Workflow utilizado}

\subsection{Pipeline y configuración} 

Así pues, esta configuración permite establecer cuales van a ser los parámetros de la pipeline, principalmente (aunque puede ser fácilmente extensible),
 la ubicación del directorio del dataset. el tamaño del dataset de entrenamiento para la creación de los modelos de redes neuronales o los valores
 de data augmentation que se aplican.

 Si se desea correr con raw, se deberá de establecer raw en la configuración del pipeline 4.

\section{\textit{data augmentation}}

También del  data augmentation, en el cual se aplica tanto ruido gaussiano (de 2.5, 5 y 7.5) como transformaciones afines (mat reflexion, mat scale, rotación y shear), siendo realizadas de 
forma aleatoria, con lo cual los supuestos pueden ser:

\begin{itemize}
    \item Imagen sin data augmentation
    \item Imagen con ruido gaussiano
    \item Imagen con transformaciones afines y 
    \item Imagen con ruido gaussiano y transformaciones afines. Este dataset será el base para todo el proyecto.
\end{itemize}

Las fases del pipeline anterior, y por lo tanto las funciones creadas en el pipeline son:
1.Tratar el dataset, de forma que las imágenes, distribuidas en distintos directorios, se guardan en uno solo. Posteriormente, se realiza en mezclado de las imágenes, el dataset se divide en dos partes, 
una que será utilizada en el proceso (70 por ciento) y, finalmente, se guarda una parte del dataset que será utilizada para obtener los resultados finales.

La segunda fase se trata de la elección de la mejor forma de clasificar las imágenes de entrada:

\section{Clasificación de las imágenes}

\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|p{3cm}|  }
    \hline
     & \multicolumn{2}{|c|}{Sin normalización} & \multicolumn{2}{|c|}{Con normalización} \\
    \hline
    Modelo& Con \textit{data augmentation} & Sin \textit{data augmentation} &Con \textit{data augmentation}&Sin \textit{data augmentation}\\
    \hline
    \textit{Accuracy} & 0.94298244   & 0.93421054    & 0.719704 &   0.679926\\
    \hline
   \end{tabular}


Estos resultados muestran, en primer lugar, que el data augmentation no supone sino un decremento del \textit{accuracy}, tanto en los casos donde se normaliza el iris
como en los que no. Esto puede deberse a que, aunque el modelo sí tenga una mayor robustez, al calcularse este parámetro utilizando datos sin ninguna modificación, que 
tienen un gran parecido a las imágenes originales, el \textit{data augmentation} disminuya ligeramente los resultados. No obstante, la mejora de la robustez previsiblemente
permitirá al sistema funcionar de forma más eficiente en un contexto no académico.

Por otro lado, en cuanto a los mejores resultados utilizando imágenes no normalizadas, se debe de tener en cuenta que, por la forma en la que funcionan las redes neuronales,
en cuanto a la reducción de imágenes para quedarse con sus características más representativas, tiene sentido que al proporcionar más elementos representativos del indivíduo,
y no solo el iris, la propia red neuronal haya sido capaz de encontrar características en la imagen que son más eficientes para su clasificación, y que, de alguna forma, son
ajenos a la zona propiamente del iris.

\subsection{Preprocesamiento y Machine Learning}

La propuesta en el TFG anterior, en la cual se realiza un preprocesamiento de las imágenes del dataset original. Este preprocesamiento consiste primer lugar 
en la segmentación de las imágenes del iris con una red neuronal pre-entrenada precisamente para realizar esta acción. En segundo lugar, se realiza una extracción
del iris a través de una binarización de las partes del ojo y una extracción del iris, a la que se le aplica una normalización para que quede proyectado.
La siguiente fase de esta primera propuesta es la extracción de características (quitándole las dos últimas capas a una red neuronal) de la imagen normalizada
 para posteriormente utilizar una red neuronal pre-entrenada con imagenet (de hecho 3 redes de la cual se elige la mejor). Posteriormente, estas características
 extraídas se pasan a modelos de ML, que son los que realizarán la clasificación. 

\subsection{Preprocesamiento y Deep Learning}

La segunda propuesta es la utilización de una red neuronal de VGG16, pre-entrenada con imagenet, a la que se le aplica un \textit{fine-tuning} del dataset de CASIAV1 para
que sea capaz de clasificar las imágenes de acuerdo a las clases establecidas en el dataset.

\subsection{Deep Learning}

La tercera propuesta se basa en la utilización del dataset, directo sin preprocesamiento, para la fase del \textit{fine-tuning} y su posterior clasificación (para la clasificación de imágenes en las fases
que han sufrido preprocesamiento, le deberían de pasar imágenes que primero hayan entrado al pipeline hasta la fase de normalización!!).
(temporal) Los resultados arrojan un acierto mayor en el caso del pre-proceso + red neuronal, por lo que se puede concluir que, en base a los parámetros establecidos en este estudio, la utilización de 
una fase de preprocesamiento que elimine las partes que los estudios determinan no tan útiles para la clasificación humana (vease, todo aquello que no es parte del iris)
 y posteriormente utiliza el \textit{fine-tuning} para adecuar una red neuronal pre-entrenada a las clases de las que se compone el dataset.

