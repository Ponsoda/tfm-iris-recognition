\capitulo{5}{Aspectos relevantes del desarrollo del proyecto} \label{capitulo5}

El proyecto se ha dividido en dos fases principales. La primera fase ha consistido en la asimilación, adecuación y optimización del código perteneciente
a \cite{tfg_iris_2020} para utilizarlo posteriormente en las fases de pre-procesamiento, estas son la segmentación y la normalización del iris. También se incluye en esta fase la aplicación de las técnicas de \textit{data augmentation}. 
Por otro lado, en la segunda fase se ha desarrollado la adaptación de la red neuronal VGG16 a los cuatro conjuntos de imágenes derivadas del \textit{dataset} creadas en la primera fase, estas son los conjuntos con/sin pre-procesamiento y con/sin \textit{data augmentation}.

El objetivo de estas dos fases, es el de crear los cuatro \textit{datasets} que se utilizarán en la fase de \textit{fine-tuning} para crear los modelos finales, con los que se calculará la tasa de acierto relativa a su capacidad de clasificación.
Estos cuatro datasets se reflejan en la tabla \ref{tabla:datasets}.

\tablaSmall{\textit{Datasets} utilizados para el \textit{fine-tuning}.}{l c c}{datasets}
{ \multicolumn{1}{l}{\textit{Dataset}} & Pre-procesamiento &  \textit{Data augmentation}\\}{ 
1 &   & \\
2 & x  & \\
3 &   & x \\
4 &  x & x \\
} 

\section{Preparación del dataset}

En los proyectos de inteligencia artificial, el procedimiento habitual para entrenar las redes neuronales es el de dividir los datos que se van a utilizar para entrenar a la red en dos subconjuntos. El subconjunto de mayor tamaño se utiliza para entrenar el modelo \emph{per se}, 
mientras que el subconjunto de menor tamaño se utiliza en la validación del mismo. 

En el caso de este proyecto caso, el \textit{dataset} inicial ha sido dividido en un 70\% para la fase de entrenamiento, y un 30\% para calcular la tasa de acierto, siendo estos los porcentajes más comunmente utilizados. 

\section{Pre-procesamiento de los datos}

En la primera fase del proyecto, se ha llevado a cabo un pre-procesamiento del \textit{dataset} para crear el conjunto de datos de imágenes del iris aisladas.

\subsection{Adecuación del código previo}

Como se ha mencionado anteriormente, el código desarrollado en \cite{tfg_iris_2020} para la segmentación y naturalización del iris en una imagen ocular ha sido adaptado para el proyecto. 

Para ello, en una primera fase se ha asimilado el código completo del trabajo, de forma que se comprendiese a fondo el mismo, para seguidamente, en una segunda fase, se ha pasado a limpiar el código y adecuarlo a las necesidades del proyecto. 

Este código permite la creación del segundo dataset utilizado en el proyecto, que siguiendo con la idea de que el iris es la mejor parte del ojo para identificar a una persona, lo aisla para entrenar a la red neuronal y evitar el ruido que pueda causar la imagen completa. 


\subsection{Aplicación del \textit{data augmentation}}

El proceso de data augmentstion permite ampliar el número de imagenes y la robustez de los dos datasets anteriormente mencionados. Estos son las imágenes ocularrs completas y las imágenes con el iris aislado. 

Por lo tanto, este proceso utiliza las tecnicas de ruido gaussiano y transformaciones afines, explicadas en la seccion \ref{transformacionesafines} para cear dos nuevos conjuntos de datos. 

Para la aplicación del ruido gaussiano, se han aplicado aleatoriamente valores de la desviacion estándar de 2.5, 5 y 7.5.

Por su lado, las transformaciones afines se han aplicado de forma independiente al ruido gaussiano, es decir, una no es excluyente de la otra, ni las transformaciones afines son excluyentes entre ellas mismas. 

Como resultado del data augmentation, los datasets, que originalmente contaban con 756 imágenes, cuentan ahora con 1158.

\subsection{Creación de la \textit{pipeline}}

Con el fin de mejorar el manejo de las diferentes secciones de código elaboradas, se ha optado por la utilización de una pipeline que permitiese controlar fácilmente la secuencia de ejecución del código así como la configuración del mismo. 

Para ello, se ha encapsulado cada uno de los procesos del proyecto en funciones independientes y se ha creado un diccionario donde se ha establecido la configuración inicial. 

Este diccionario era el único elemento de entrada y salida en la pipeline y permitía que en cada proceso se pudiese modificar o ampliar el diccionario, haciendo posible el cambio del orden de los procesos en el pipeline. 

Los criterios de la configuración de la pipeline se muestran en la siguiente tabla \ref{tabla:configuracion-pipeline}.

\tablaSmallSinColores{Configuración del \textit{pipeline}}{l c }{configuracion-pipeline}
{ \multicolumn{1}{l}{Nombre} & Configuración \\}{ 
Raíz & Establecimiento del directorio raíz\\
Tamaño dataset & Tamaño de los \textit{datasets} de entrenamiento y validación\\
\textit{Data augmentation} & Tipo de \textit{data augmentation} a aplicar\\
Imágenes y gráficas & \makecell{Mostrar imágenes y gráficas, configurado individualmente \\ para cada elemento.}\\
Modelo &  Nombre y ubicación donde guardar la red neuronal.\\
Epochs & Número de epochs por cada red neuronal entrenada.\\
\textit{Batch} & Tamaño del \textit{batch}.\\
\textit{Random seed} & Tamaño del \textit{random seed}.\\
Peso & Peso utilizado para el entrenamiento la red neuronal.\\
\textit{Fine-tuning} & \textit{Dataset} ha utilizar para el \textit{fine-tuning}.\\
} 
  
\section{Adaptación de la red neuronal}

La adaptación de la red neuronal VGG16 es una parte central del proyecto, puesto que el objetivo principal del proyecto es comparar como comporta la red neuronal al ser adaptada por uno de los datasets desarrollados en el proyecto, en cuanto a tasa de acierto en la clasificación de individuos a través de sus imágenes oculares. 

La adaptación de la red neuronal se ha aplicado a los cuatro \textit{datasets}, estos son, por un lado, lo dos \textit{datasets} sin pre-procesamiento (uno de ellos con \textit{data augmentation}) y, por otro lado, a los dos datasets a los que se les ha aplicado el pre-procesamiento (de nuevo, a uno de ellos también se le han aplicado técnicas de \textit{data augmentation}). 
\subsection{Aplicación del \textit{fine-tuning}}

En la  utilización de técnicas de fine-tuning para la adaptación de la red neuronal VGG16 a los datasets del proyecto, se han seguido tres fases. 

\subsubsection{Primera fase}
En una primera fase, se ha definido una primera red neuronal cuyo objetivo es el de determinar las características que mejor definen a las imágenes, siendo la base del nuevo modelo. 

Para definir esta red neuronal, se ha eliminado la capa final, que corresponde a la clasificación de la imagen, a una red neuronal que entrenada con ImageNet, menos la capa final, puesto que el modelo será utilizado como modelo base. El modelo se establece en este punto como no entrenable para que no se re-entrene en las siguiente fase, de forma que el modelo resultante funcione como un \textit{inference model}\footnote{Este modelo aprovecha el conocimiento de una red neuronal ya entrenada para clasificar imágenes pero interfiere en el resultado final, que es modificado por un nuevo \textit{dataset}, que suele ser de un tamaño no lo suficientemente grande para entrenar la red neuronal desde cero.}. 

\subsubsection{Segunda fase}

En la segunda fase, se ha creado una nueva capa a partir de el modelo base , que es capaz de clasificar a las imágenes en base a las imágenes oculares y sus etiquetas. 

\subsubsection{Tercera fase}

En esta tercera fase, se pasa a entrena el modelo completo, descongelando el modelo base pero manteniendolo como \textit{inference model} para evitar que se vuelta a entrenar,  por lo que la extradición de características se hará sobre el nuevo \textit{dataset} pero las textit {hidden layers} utilizadas serán principalmente las de VGG16.

\subsection{Clasificación de imágenes}

Las imagenes se han clasificado utilizando los cuatro modelos creados en la sección anterior. Para ello, utilizando como dataset de entrada, el 30\% reservado en la primera fase.

En el caso las imágenes normalizadas, antes de ser clasificars, se les aplica la fase de pre-procesamiento \ref{preprocesamiento}, de forma que estas se ajusten a la entrada requerida por el modelo.

\subsection{Tasa de acierto} 

La tasa de acierto depresenta el número de veces que el modelo a determinado correctamente la clase de la imagen. 

En la siguiente tabla \ref{tabla:tasa-acierto-modelos}, se puede observar la tasa de acierto que han tenido los modelos, a la hora de relacionar las imágenes
 con los indivíduos.

\begin{table}[h!]
\begin{tabular}{ |p{2cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
     & \multicolumn{2}{|c|}{Sin normalización} & \multicolumn{2}{|c|}{Con normalización} \\
    \hline
    Modelo& Con \textit{data augmentation} & Sin \textit{data augmentation} &Con \textit{data augmentation}&Sin \textit{data augmentation}\\
    \hline
    \textit{Accuracy} & 0.94   & 0.93    & 0.72 &   0.68\\
    \hline
   \end{tabular}
   \caption{\label{tabla:tasa-acierto-modelos}Tasa de acierto de los modelos.}
\end{table}


Estos resultados muestran, en primer lugar, que el data augmentation no supone sino un decremento del \textit{accuracy}, tanto en los casos donde se normaliza el iris
como en los que no. Esto puede deberse a que, aunque el modelo sí tenga una mayor robustez, al calcularse este parámetro utilizando datos sin ninguna modificación, que 
tienen un gran parecido a las imágenes originales, el \textit{data augmentation} disminuya ligeramente los resultados. No obstante, la mejora de la robustez previsiblemente
permitirá al sistema funcionar de forma más eficiente en un contexto no académico.

Por otro lado, en cuanto a los mejores resultados utilizando imágenes no normalizadas, se debe de tener en cuenta que, por la forma en la que funcionan las redes neuronales,
en cuanto a la reducción de imágenes para quedarse con sus características más representativas, tiene sentido que al proporcionar más elementos representativos del indivíduo,
y no solo el iris, la propia red neuronal haya sido capaz de encontrar características en la imagen que son más eficientes para su clasificación, y que, de alguna forma, son
ajenos a la zona propiamente del iris.

Además, cabe de tener en cuenta que, tal como se explica en \nameref{casia}, el \textit{dataset} utilizado ha sido sometido a un preprocesamiento previo, en el que se elimió la pupila 
para evitar que el brillo emitido por las cámaras para tomer las propias imágenes pudiera afectar a la misma. Por lo tanto, realmente la normalización practicada solo ha afectado a la 
parte exterior al iris, y esto puede haber reducido su efecto.

