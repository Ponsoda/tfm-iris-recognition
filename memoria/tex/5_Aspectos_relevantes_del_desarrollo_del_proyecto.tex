\capitulo{5}{Aspectos relevantes del desarrollo del proyecto} \label{capitulo5}

El proyecto se ha dividido en dos fases principales. La primera fase, ha consistido primeramente en la asimilación, adecuación y optimización del código perteneciente
al proyecto de partida \cite{tfg_iris_2020}, así como en la aplicación de las técnicas de \textit{data augmentation}, para posteriormente poder utilizar el proceso de normalización del iris en la segunda fase. 
Por otro lado, en la segunda fase se ha desarrollado la personalización de la red neuronal VGG16 a las imágenes (tanto normalizadas como no procesadas) del \textit{dataset} utilizado en el proyecto.


\section{Preparación del dataset}

El dataset inicial, se ha dividido en un 70\% para llevar a cabo el proyecto, y un 30\% para comprobar la tasa de acierto del modelo para su clasificación. 
Este dataset del 70\% será el que se utilizará para la creación de modelos, dividiéndose a su ver en 70-30 para ello. 

\section{Preprocesamiento de los datos}

En la primera fase del proyecto, se ha llevado a cabo un procesamiento de datos previo, que se explicará a continuación .

\subsection{Adecuación del código previo}

En el código previo\footnote{Accesible desde \url{https://github.com/jaa0124/iris_classifier}.}, correspondiente a \cite{tfg_iris_2020} cuenta con un código donde el iris se segmenta y se normaliza.

Durante esta fase, se han llevado a cabo tres aspectos. La primera ha sido la asimilación del \textit{dataset} y del código previo. La segunda ha sido la adecuación del código previo para que fuese funcional,

y seguidamente, se ha llevado a cabo una asimilación del código. 

\subsection{Creación de la \textit{pipeline}}

Para el manejo del Código, se ha encapsulado cada sección en el pipeline, y se ha creado un diccionario de configuración, donde se establecen los criterios que se muestran en la tabla \ref{tabla:configuracion-pipeline}.

%tabla con el nombre y lo que hace la configuración/razón de ser

\tablaSmall{Configuración del \textit{pipeline}}{l c }{configuracion-pipeline}
{ \multicolumn{1}{l}{Nombre} & Configuración \\}{ 
Raíz & Establecimiento del directorio raíz\\
Tamaño dataset & Tamaño de los \textit{datasets} de entrenamiento y validación\\
\textit{Data augmentation} & Tipo a aplicar\\
Imágenes y gráficas &  Mostrar imágenes y gráficas (configurado individualmente)\\
Modelo &  Nombre y ubicación donde guardar la red neuronal\\
Epochs & Número de epochs por cada red neuronal entrenada\\
\textit{Batch} & Tamaño\\
\textit{Random seed} & Tamaño\\
Peso & Peso utilizado para la red neuronal\\
\textit{Fine-tuning} & \textit{Dataset} utilizado\\
} 
  

\subsection{Aplicación del \textit{data augmentation} sobre el dataset}

La aplicación del data augmentation se ha hecho de forma alternativa, primero con el ruido gaussiano, donde aleatoriamente se aplica ruido gaussiano de 2.5, 5 y 7.5 de desviación. 

Además, con las transformaciones afines, que no son excluyentes del ruido gaussiano, ni entre ellas mismas, se aplican aleatoriamente entre cada una de ellas. 

Como resultado, los datasets con data augmentation aplicado cuentan con un total de x imágenes, y más que el dataset original, con una media de z más imagenes por individuo. 

\section{Adaptación de la red neuronal}

En la configuración del pipeline se establece a que dataset se aplica el \textit{fine-tuning}. En este caso se ha aplicado a cuatro datasets. 
Por un lado, a dos datasets sin preprocesamiento, uno condata augmentation y otro sin. Por otro lado, a dos datasets con el preprocesamiento, uno con data augmentation y otro sin. 

\subsection{Aplicación del \textit{fine-tuning}}

%Explicar xomo se splica, con el ejemplo de las tres cnn https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
%https://stackoverflow.com/questions/54059391/why-we-include-top-false-while-using-pretrained-models-like-inceptionresnetv2-in

Para llevar a cabo la aplicación del \textit{fine-tuning}, se han creado 3 redes neuronales.

\subsubsection{Primera fase}

%https://mitxpc.com/pages/ai-inference-applying-deep-neural-network-training

En primer lugar, se ha creado una primera red neuronal que permita encontrar las características principales de las imágenes.
Para ello, se ha eliminado la capa final, correspondiente a la clasificación de la imagen.
A la hora de crear el modelo, se establece como `training=false` para que funcione como un \textit{interference model}.

A la hora de lanzar este primer modelo, que será el modelo base para la red neuronal final, se establece como  ´traning=false´, para mantenerlo en \textit{inference mode}.


\subsubsection{Segunda fase}

Para la segunda fase, se ha entrenado una nueva capa, que será la que determine el \textit{output}, para luego hacer un \textit{unfreeze} del modelo creado anteriormente.

\subsubsection{Tercera fase}

En esta tercera fase, se entrena el modelo completo, manteniendo al modelo base como \textit{iterference model} para evitar que se vuelta a entrenar.


\subsection{Clasificación de imágenes}

Las imagenes se han clasificado utilizando los modelos creados en la sección anterior, utilizando como dataset de entrada, el 30\% reservado en la primera fase. 

Para las imágenes normalizadas, se les aplica la fase de preprocesamiento, para que puedan ser manejadas por el modelo. 

\subsection{Tasa de acierto} 

La tasa de acierto depresenta el número de veces que el modelo a determinado correctamente la clase de la imagen. 

En la siguiente tabla \ref{tabla:tasa-acierto-modelos}, se puede observar la tasa de acierto que han tenido los modelos, a la hora de relacionar las imágenes
 con los indivíduos.

\begin{table}[h!]
\begin{tabular}{ |p{2cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
     & \multicolumn{2}{|c|}{Sin normalización} & \multicolumn{2}{|c|}{Con normalización} \\
    \hline
    Modelo& Con \textit{data augmentation} & Sin \textit{data augmentation} &Con \textit{data augmentation}&Sin \textit{data augmentation}\\
    \hline
    \textit{Accuracy} & 0.94   & 0.93    & 0.72 &   0.68\\
    \hline
   \end{tabular}
   \caption{\label{tabla:tasa-acierto-modelos}Tasa de acierto de los modelos.}
\end{table}


Estos resultados muestran, en primer lugar, que el data augmentation no supone sino un decremento del \textit{accuracy}, tanto en los casos donde se normaliza el iris
como en los que no. Esto puede deberse a que, aunque el modelo sí tenga una mayor robustez, al calcularse este parámetro utilizando datos sin ninguna modificación, que 
tienen un gran parecido a las imágenes originales, el \textit{data augmentation} disminuya ligeramente los resultados. No obstante, la mejora de la robustez previsiblemente
permitirá al sistema funcionar de forma más eficiente en un contexto no académico.

Por otro lado, en cuanto a los mejores resultados utilizando imágenes no normalizadas, se debe de tener en cuenta que, por la forma en la que funcionan las redes neuronales,
en cuanto a la reducción de imágenes para quedarse con sus características más representativas, tiene sentido que al proporcionar más elementos representativos del indivíduo,
y no solo el iris, la propia red neuronal haya sido capaz de encontrar características en la imagen que son más eficientes para su clasificación, y que, de alguna forma, son
ajenos a la zona propiamente del iris.

Además, cabe de tener en cuenta que, tal como se explica en \nameref{casia}, el \textit{dataset} utilizado ha sido sometido a un preprocesamiento previo, en el que se elimió la pupila 
para evitar que el brillo emitido por las cámaras para tomer las propias imágenes pudiera afectar a la misma. Por lo tanto, realmente la normalización practicada solo ha afectado a la 
parte exterior al iris, y esto puede haber reducido su efecto.

