{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "confi_dict = {\r\n",
    "\r\n",
    "    'general':{\r\n",
    "        'root_dir':r\"C:\\Users\\na-ch\\Desktop\\estudio\\Master_Big_Data\\03_TFM\\02_Code\\dataset_test\",\r\n",
    "        'dataset_dir': \"CASIA_IrisV1\"\r\n",
    "    },\r\n",
    "\r\n",
    "    '1_tratarDataset':{\r\n",
    "        'show_first' : False\r\n",
    "    },\r\n",
    "\r\n",
    "    '1.1_dataAugmentation':{\r\n",
    "        'gaussianNoise' : False,\r\n",
    "        'stdGN': [5, 10, 15, 20],\r\n",
    "        'skipFiles': 1, \r\n",
    "        ''' \r\n",
    "        1 means that you are skiping 1 for the noise augmentation, \r\n",
    "        2 skiping 2 and so on. If you don't want to skip files set it as 0\r\n",
    "        '''\r\n",
    "        'afinTransformation': False\r\n",
    "    },\r\n",
    "\r\n",
    "    '2.1_segmentation':{\r\n",
    "        'redNeuronal' : \"Iris_unet_d5.h5\", \r\n",
    "        'numSamplesGenerator' : 3, \r\n",
    "        'verImagenV1' : False\r\n",
    "    }\r\n",
    "\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1: Tratar el dataset CASIA\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 tratar_dataset_casia()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def tratar_dataset_casia(dict):\r\n",
    "    \r\n",
    "    print(\"Función 1, tratar_dataset_casia()\")\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    try:\r\n",
    "        import shutil\r\n",
    "    except:\r\n",
    "        !pip install shutil\r\n",
    "        import shutil\r\n",
    "    try:\r\n",
    "         import imageio\r\n",
    "    except:\r\n",
    "        !pip install imageio\r\n",
    "        import imageio\r\n",
    "    try:\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    except:\r\n",
    "        !pip install matplotlib\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "    \r\n",
    "    def copy_all_samples(path_samples, destination_directory):\r\n",
    "        '''\r\n",
    "        Copia todas las muestras del dataset de CASIA a un único directorio.\r\n",
    "        '''\r\n",
    "        if not os.path.exists(path_samples):\r\n",
    "            print(\"->El directorio indicado como input no existe\")\r\n",
    "        elif not os.path.exists(destination_directory):\r\n",
    "            ''' \r\n",
    "            Comprobamos que el directorio del output no existe\r\n",
    "            '''\r\n",
    "            os.mkdir(destination_directory) # Creamos un nuevo directorio \r\n",
    "            for root, dirs, files in os.walk(path_samples):\r\n",
    "                for file in files:\r\n",
    "                    path_file = os.path.join(root,file)\r\n",
    "                    shutil.copy2(path_file,destination_directory)   \r\n",
    "            print(\"-> Muestras copiadas satisfactoriamente.\")\r\n",
    "        elif len(os.listdir(destination_directory)) < 1:\r\n",
    "            ''' \r\n",
    "            Comprobamos que el directorio del output no está vacío\r\n",
    "            '''\r\n",
    "            for root, dirs, files in os.walk(path_samples):\r\n",
    "                for file in files:\r\n",
    "                    path_file = os.path.join(root,file)\r\n",
    "                    shutil.copy2(path_file,destination_directory)   \r\n",
    "            print(\"-> Muestras copiadas satisfactoriamente.\")\r\n",
    "        else:\r\n",
    "            print(\"-> Muestras previamente copiadas.\")\r\n",
    "\r\n",
    "    dict['general']['dataset_unif_dir'] = dict['general']['dataset_dir']+\"_unif\"\r\n",
    "\r\n",
    "    copy_all_samples(dict['general']['dataset_dir'], dict['general']['dataset_unif_dir'])\r\n",
    "\r\n",
    "    if dict['1_tratarDataset']['show_first']:\r\n",
    "        ''' \r\n",
    "        Si en la configuración show_first es True, se mostrará el primer elemento del nuevo directorio.\r\n",
    "        '''\r\n",
    "        path_img = dict['general']['dataset_unif_dir'] + os.sep + os.listdir(dict['general']['dataset_unif_dir'])[0]\r\n",
    "        img = imageio.imread(path_img)\r\n",
    "        plt.title(\"Primer elemento de \" + dict['general']['dataset_unif_dir'])\r\n",
    "        plt.imshow(img)\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Pasando el diccionario con la configuración a la siguiente función\r\n",
    "    '''\r\n",
    "    return dict\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 data_augmentation()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def data_augmentation(dict):\r\n",
    "\r\n",
    "    print(\"Función 1.1, data_augmentation()\")\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    try:\r\n",
    "        import imageio\r\n",
    "    except:\r\n",
    "        !pip install imageio\r\n",
    "        import imageio\r\n",
    "    try:\r\n",
    "        import skimage.io as io\r\n",
    "    except:\r\n",
    "        !pip install scikit-image\r\n",
    "        import skimage.io as io\r\n",
    "    try:\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    except:\r\n",
    "        !pip install matplotlib\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    try:\r\n",
    "        from tensorflow.keras.layers import GaussianNoise\r\n",
    "    except:\r\n",
    "        !pip install tensorflow\r\n",
    "        from tensorflow.keras.layers import GaussianNoise\r\n",
    "    try:\r\n",
    "        import numpy as np\r\n",
    "    except:\r\n",
    "        !pip install numpy\r\n",
    "        import numpy as np\r\n",
    "    import random\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Comprobamos si el ruido gaussiano ha sido establecido como True en la configuración\r\n",
    "    '''\r\n",
    "\r\n",
    "    if dict['1.1_dataAugmentation']['gaussianNoise']:\r\n",
    "\r\n",
    "        ''' \r\n",
    "        Comprobamos si el ruido gaussiano se ha aplicado anteriormente, si es así, el primer elemento del directorio debería de tener el sufijo _augmentation\r\n",
    "        '''\r\n",
    "        if \"_augment\" in os.listdir(dict['general']['dataset_unif_dir'])[1]:\r\n",
    "            print(\"->El ruido gaussiano ya se ha ejecutado con anterioridad\")\r\n",
    "        else:     \r\n",
    "            ''' \r\n",
    "            Desde el directorio unificado creamos nuevas imágenes utilizando el ruido gaussiano como data augmentation.\r\n",
    "            Añadimos un contador para que el augmentation sea de 1 cada dos imágenes.\r\n",
    "            '''\r\n",
    "            i = 0\r\n",
    "            for filename in os.listdir(dict['general']['dataset_unif_dir']):\r\n",
    "                if dict['1.1_dataAugmentation']['skipFiles'] == 0:\r\n",
    "                    path = dict['general']['dataset_unif_dir'] + os.sep + filename\r\n",
    "                    image = imageio.imread(path)/255\r\n",
    "                    std_rdm = random.choice(dict['1.1_dataAugmentation']['stdGN'])\r\n",
    "                    gaussean_function = GaussianNoise(std_rdm/100)\r\n",
    "                    noisey = gaussean_function(image.astype(np.float32),training=True)\r\n",
    "                    name = path[:-4] + '_augment.bmp'\r\n",
    "                    plt.imsave(name, noisey, cmap='gray')\r\n",
    "                elif i < dict['1.1_dataAugmentation']['skipFiles']:\r\n",
    "                    path = dict['general']['dataset_unif_dir'] + os.sep + filename\r\n",
    "                    image = imageio.imread(path)/255\r\n",
    "                    std_rdm = random.choice(dict['1.1_dataAugmentation']['stdGN'])\r\n",
    "                    gaussean_function = GaussianNoise(std_rdm/100)\r\n",
    "                    noisey = gaussean_function(image.astype(np.float32),training=True)\r\n",
    "                    name = path[:-4] + '_augment.bmp'\r\n",
    "                    io.imsave(name, noisey)\r\n",
    "                    i += 1\r\n",
    "                else:\r\n",
    "                    i = 0\r\n",
    "            print(\"->Data augmentation 'ruido gausseano' ejecutado correctamente\")\r\n",
    "    else:\r\n",
    "        print(\"->Saltando el ruido gausseano en data augmentation\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Pasando el diccionario con la configuración a la siguiente función\r\n",
    "    '''\r\n",
    "    return dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Red U-Net v2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 segmentation()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def segmentation(dict):\r\n",
    "\r\n",
    "    print(\"Función 2.1, segmentation()\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    try:\r\n",
    "        import cv2\r\n",
    "    except:\r\n",
    "        import cv2\r\n",
    "    try:\r\n",
    "        import shutil\r\n",
    "    except:\r\n",
    "        !pip install shutil\r\n",
    "        import shutil\r\n",
    "    try:\r\n",
    "         import imageio\r\n",
    "    except:\r\n",
    "        !pip install imageio\r\n",
    "        import imageio\r\n",
    "    try:\r\n",
    "        import skimage.transform as trans\r\n",
    "        from skimage.io import imshow\r\n",
    "        import skimage.io as io\r\n",
    "    except:\r\n",
    "        !pip install scikit-image\r\n",
    "        import skimage.transform as trans\r\n",
    "        from skimage.io import imshow\r\n",
    "        import skimage.io as io\r\n",
    "    try:\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    except:\r\n",
    "        !pip install matplotlib\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    try:\r\n",
    "        from keras.models import load_model\r\n",
    "    except:\r\n",
    "        !pip install tensorflow \r\n",
    "        from keras.models import load_model\r\n",
    "    try:\r\n",
    "        import numpy as np\r\n",
    "    except:\r\n",
    "        !pip install numpy\r\n",
    "        import numpy as np\r\n",
    "    try:\r\n",
    "        from PIL import Image\r\n",
    "    except:\r\n",
    "        !pip install Pillow\r\n",
    "        from PIL import Image\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Creamos un generador con un número n de imágenes para pasarselas al modelo en el predict\r\n",
    "    *Recordemos que este modelo ya está entrenado para la segmentación del ojo\r\n",
    "    '''\r\n",
    "    def testGenerator(directory, target_size = (320,320)):\r\n",
    "        '''\r\n",
    "        Genera las muestras que se le pasarán al método predecir de la red preentrenada.\r\n",
    "        \r\n",
    "        Parámetros:\r\n",
    "        directory -- directorio dónde se encuentran las muestras a generar\r\n",
    "        num_image -- número máximo de imágenes que queremos generar\r\n",
    "        target_size -- shape de de la muestra\r\n",
    "        flag_multi_class -- (de momento mantener pero no nos es útil)\r\n",
    "        '''\r\n",
    "        for file in os.listdir(directory):\r\n",
    "            path_file = os.path.join(directory,file)\r\n",
    "            img = imageio.imread(path_file)\r\n",
    "            img = img / 255\r\n",
    "            img = trans.resize(img,target_size)\r\n",
    "            img = np.reshape(img, img.shape+(1,))\r\n",
    "            img = np.reshape(img, (1,) + img.shape)\r\n",
    "            yield img\r\n",
    "    \r\n",
    "    def get_samples_names(directory):\r\n",
    "        '''\r\n",
    "        Devuelve los nombres de las muestras (los que se encuentran en los directorios hoja)\r\n",
    "        \r\n",
    "        Parámetros:\r\n",
    "        directory -- directorio de cuyos subdirectorios hoja se quiere el nombre \r\n",
    "        '''\r\n",
    "        names = []\r\n",
    "        for file in os.listdir(directory):\r\n",
    "            names.append(str(file))\r\n",
    "                \r\n",
    "        return names\r\n",
    "\r\n",
    "    def saveResult(save_path, name_path, npyfile):\r\n",
    "        '''\r\n",
    "        Guarda los imágenes segmentadas resultado de la red preentrenada\r\n",
    "        \r\n",
    "        Parámetros:\r\n",
    "        save_path -- ruta en la que se guardarán las muestras segmentadas\r\n",
    "        name_path -- directorio del que cogeremos los nombres de las muestras\r\n",
    "        npyfile -- resultado de la red preentrenada\r\n",
    "        '''\r\n",
    "        dim = (320, 280) # dimensioned de las muestras del dataset\r\n",
    "        os.mkdir(save_path)\r\n",
    "        names = get_samples_names(name_path)\r\n",
    "        for i, item in enumerate(npyfile):\r\n",
    "            img = item[:,:,0]\r\n",
    "            img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\r\n",
    "            io.imsave(os.path.join(save_path,names[i]),img)    \r\n",
    "\r\n",
    "    dict['general']['dataset_unif_seg_dir'] =  dict['general']['dataset_unif_dir']  + \"_seg\"\r\n",
    "    if not os.path.exists(dict['general']['dataset_unif_seg_dir']):\r\n",
    "        ''' \r\n",
    "        Cargamos el modelo de segmentación\r\n",
    "        '''\r\n",
    "        model = load_model(dict['2.1_segmentation']['redNeuronal'])\r\n",
    "        print(\"->Modelo cargado\")\r\n",
    "        testGene = testGenerator(dict['general']['dataset_unif_dir'])\r\n",
    "        Unet_results = model.predict(testGene, verbose=1) \r\n",
    "        saveResult(dict['general']['dataset_unif_seg_dir'], dict['general']['dataset_unif_dir'], Unet_results)\r\n",
    "        print(\"->Segmentaciones guardadas\")\r\n",
    "    else:\r\n",
    "        print(\"->Segmentaciones previamente guardadas\")\r\n",
    "\r\n",
    "    if dict['2.1_segmentation']['verImagenV1']:\r\n",
    "        sample = \"001_1_1.bmp\"\r\n",
    "        path_img = dict['general']['dataset_unif_dir'] + os.sep + sample\r\n",
    "        path_back = dict['general']['dataset_unif_seg_dir'] + os.sep + sample\r\n",
    "        image = Image.open(path_img)\r\n",
    "        background = Image.open(path_back)\r\n",
    "        background.paste(image, (0, 0), image)\r\n",
    "        plt.imshow(background, cmap='gray');\r\n",
    "    \r\n",
    "    dict['general']['dataset_unif_segv2_dir'] = dict['general']['dataset_unif_seg_dir'] + \"v2\"\r\n",
    "    dict['general']['dataset_unif_segv2_edg_dir'] = dict['general']['dataset_unif_segv2_dir'] + \"_edg\"\r\n",
    "\r\n",
    "    segv2_dir = False\r\n",
    "    edge_dir = False\r\n",
    "\r\n",
    "    for file in os.listdir(dict['general']['dataset_unif_seg_dir']):\r\n",
    "        path_file = os.path.join(dict['general']['dataset_unif_seg_dir'],file)\r\n",
    "        img = cv2.imread(path_file)\r\n",
    "        blur= cv2.GaussianBlur(img,(17,17),0)\r\n",
    "        (thresh, binarized) = cv2.threshold(blur, 70, 255, cv2.THRESH_BINARY) # se binariza nuevamente\r\n",
    "        edges = cv2.Canny(binarized, 10, 255)\r\n",
    "\r\n",
    "        # creamos un nuevo directorio para guardar las muestras segmentadas con mayor calidad\r\n",
    "        if not os.path.exists(dict['general']['dataset_unif_segv2_dir']):\r\n",
    "            os.mkdir(dict['general']['dataset_unif_segv2_dir'])\r\n",
    "            cv2.imwrite(dict['general']['dataset_unif_segv2_dir'] + os.sep + file, binarized)\r\n",
    "            segv2_dir = True\r\n",
    "        elif segv2_dir:\r\n",
    "            cv2.imwrite(dict['general']['dataset_unif_segv2_dir'] + os.sep + file, binarized)\r\n",
    "        else:\r\n",
    "            print(\"->La segmentación v2 ya había sido creada\")\r\n",
    "            break \r\n",
    "\r\n",
    "        # creamos un nuevo directorio para guardar las muestras con los bordes detectados\r\n",
    "        if not os.path.exists(dict['general']['dataset_unif_segv2_edg_dir']):\r\n",
    "            os.mkdir(dict['general']['dataset_unif_segv2_edg_dir'])\r\n",
    "            cv2.imwrite(dict['general']['dataset_unif_segv2_edg_dir'] + os.sep + file, edges)# guardamos muestra\r\n",
    "            edge_dir = True\r\n",
    "        elif edge_dir:\r\n",
    "            cv2.imwrite(dict['general']['dataset_unif_segv2_edg_dir'] + os.sep + file, edges)# guardamos muestra\r\n",
    "        else:\r\n",
    "            print(\"->La edge ya había sido creada\")\r\n",
    "            break \r\n",
    "    \r\n",
    "    print(\"->Segmentaciones V2 guardadas\")\r\n",
    "    print(\"->Edge guardadas\")\r\n",
    "\r\n",
    "    return dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 normalization()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def normalization(dict):\r\n",
    "\r\n",
    "    print(\"Función 2.2, normalization()\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    try:\r\n",
    "        import cv2\r\n",
    "    except:\r\n",
    "        import cv2\r\n",
    "    try:\r\n",
    "        from skimage.transform import (hough_circle, hough_circle_peaks)\r\n",
    "        from skimage.io import imshow\r\n",
    "    except:\r\n",
    "        !pip install scikit-image\r\n",
    "        from skimage.transform import (hough_circle, hough_circle_peaks)\r\n",
    "        from skimage.io import imshow\r\n",
    "    try:\r\n",
    "         import imageio\r\n",
    "    except:\r\n",
    "        !pip install imageio\r\n",
    "        import imageio\r\n",
    "    try:\r\n",
    "        import numpy as np\r\n",
    "    except:\r\n",
    "        !pip install numpy\r\n",
    "        import numpy as np\r\n",
    "    try:\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    except:\r\n",
    "        !pip install matplotlib\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "\r\n",
    "    def draw_circles(img, cx, cy, radii):\r\n",
    "        '''\r\n",
    "        A partir de los centros y el radio detectados dibuja el iris sobre la imagen que se le\r\n",
    "        pasa como parámetro.\r\n",
    "        '''\r\n",
    "        image = img.copy()\r\n",
    "        pupil = cv2.circle(image,(cx[0],cy[0]), radii[0]+3, (255, 0, 0), 2)\r\n",
    "        iris = cv2.circle(image,(cx[1],cy[1]), radii[1], (255, 0, 0), 2)\r\n",
    "        return image\r\n",
    "    \r\n",
    "    def get_circles(borde, sample):\r\n",
    "        path_sample = \"CASIA_IrisV1_unif\" + os.sep + sample\r\n",
    "        path_edged = \"CASIA_IrisV1_unif_segv2_edg\" + os.sep + sample\r\n",
    "        sample_ = imageio.imread(path_sample)\r\n",
    "        gray_img = imageio.imread(path_edged)\r\n",
    "        \r\n",
    "        hough_radii = np.arange(20, 80) # pupila por defecto\r\n",
    "        if borde == \"iris\":\r\n",
    "            hough_radii = np.arange(90, 160) # rango del iris\r\n",
    "        \r\n",
    "        hough_res = hough_circle(gray_img, hough_radii)\r\n",
    "        accums, cx, cy, radii = hough_circle_peaks(hough_res, hough_radii, total_num_peaks=1) \r\n",
    "\r\n",
    "        \r\n",
    "        return [cx[0], cy[0], radii[0]]\r\n",
    "\r\n",
    "    dict['general']['dataset_unif_segv2_edg_norm_dir'] = dict['general']['dataset_unif_segv2_edg_dir'] + \"_norm\"\r\n",
    "    \r\n",
    "    os.mkdir(dict['general']['dataset_unif_segv2_edg_norm_dir'])\r\n",
    "\r\n",
    "    for file in os.listdir(dict['general']['dataset_unif_segv2_dir']):\r\n",
    "        \r\n",
    "        try:\r\n",
    "            path_sample = \"CASIA_IrisV1_unif\" + os.sep + file\r\n",
    "            sample_image = imageio.imread(path_sample)\r\n",
    "            boundaries,centers = [],[]\r\n",
    "            pupil_coord = get_circles(\"pupil\",file)\r\n",
    "            iris_coord = get_circles(\"iris\",file)\r\n",
    "            cx, cy, radius = list(zip(pupil_coord, iris_coord))\r\n",
    "            draw = draw_circles(sample_image,cx,cy,radius)\r\n",
    "\r\n",
    "            boundaries.append(draw)\r\n",
    "            centers.append(pupil_coord)\r\n",
    "\r\n",
    "            target = [img for img in boundaries]\r\n",
    "            normalized=[]\r\n",
    "            cent=0\r\n",
    "            \r\n",
    "            for img in target:\r\n",
    "                #load pupil centers and radius of inner circles\r\n",
    "                center_x = centers[cent][0]\r\n",
    "                center_y = centers[cent][1]\r\n",
    "                radius_pupil=int(centers[cent][2])\r\n",
    "\r\n",
    "                iris_radius = 53 # width of space between inner and outer boundary\r\n",
    "\r\n",
    "                #define equally spaced interval to iterate over\r\n",
    "                nsamples = 360\r\n",
    "                samples = np.linspace(0,2*np.pi, nsamples)[:-1]\r\n",
    "                polar = np.zeros((iris_radius, nsamples))\r\n",
    "                for r in range(iris_radius):\r\n",
    "                    for theta in samples:\r\n",
    "                        #get x and y for values on inner boundary\r\n",
    "                        x = (r+radius_pupil)*np.cos(theta)+center_x\r\n",
    "                        y = (r+radius_pupil)*np.sin(theta)+center_y\r\n",
    "                        x=int(x)\r\n",
    "                        y=int(y)\r\n",
    "                        try:\r\n",
    "                        #convert coordinates\r\n",
    "                            polar[r][int((theta*nsamples)/(2*np.pi))] = img[y][x]\r\n",
    "                        except IndexError: #ignores values which lie out of bounds\r\n",
    "                            pass\r\n",
    "                        continue\r\n",
    "                res = cv2.resize(polar,(512,64))\r\n",
    "                normalized.append(res)\r\n",
    "                cent+=1\r\n",
    "\r\n",
    "            h,w = normalized[0].shape # height, width\r\n",
    "\r\n",
    "            roi = normalized[0][10:h, 0:int(512/2)]\r\n",
    "            roi = roi.astype(np.uint8) \r\n",
    "            roi_enhanced = cv2.equalizeHist(roi)\r\n",
    "            cv2.imwrite(dict['general']['dataset_unif_segv2_edg_norm_dir'] + os.sep + file, roi_enhanced)\r\n",
    "        except:\r\n",
    "            print(\"->Ha ocurrido un error durante la etapa de normalización\")\r\n",
    "\r\n",
    "            \r\n",
    "    \r\n",
    "    return dict\r\n",
    "\r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def normalization(dict):\r\n",
    "\r\n",
    "    print(\"Función 2.2, normalization()\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    try:\r\n",
    "        import cv2\r\n",
    "    except:\r\n",
    "        import cv2\r\n",
    "    try:\r\n",
    "        from skimage.transform import (hough_circle, hough_circle_peaks)\r\n",
    "        from skimage.io import imshow\r\n",
    "    except:\r\n",
    "        !pip install scikit-image\r\n",
    "        from skimage.transform import (hough_circle, hough_circle_peaks)\r\n",
    "        from skimage.io import imshow\r\n",
    "    try:\r\n",
    "         import imageio\r\n",
    "    except:\r\n",
    "        !pip install imageio\r\n",
    "        import imageio\r\n",
    "    try:\r\n",
    "        import numpy as np\r\n",
    "    except:\r\n",
    "        !pip install numpy\r\n",
    "        import numpy as np\r\n",
    "    try:\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    except:\r\n",
    "        !pip install matplotlib\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    try:\r\n",
    "        import pandas as pd\r\n",
    "    except:\r\n",
    "        !pip install pandas\r\n",
    "        import pandas as pd\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "\r\n",
    "    def draw_circles(img, cx, cy, radii):\r\n",
    "        '''\r\n",
    "        A partir de los centros y el radio detectados dibuja el iris sobre la imagen que se le\r\n",
    "        pasa como parámetro.\r\n",
    "        '''\r\n",
    "        image = img.copy()\r\n",
    "        pupil = cv2.circle(image,(cx[0],cy[0]), radii[0]+3, (255, 0, 0), 2)\r\n",
    "        iris = cv2.circle(image,(cx[1],cy[1]), radii[1], (255, 0, 0), 2)\r\n",
    "        return image\r\n",
    "    \r\n",
    "    def get_circles(borde, sample):\r\n",
    "        path_sample = \"CASIA_IrisV1_unif\" + os.sep + sample\r\n",
    "        path_edged = \"CASIA_IrisV1_unif_segv2_edg\" + os.sep + sample\r\n",
    "        sample_ = imageio.imread(path_sample)\r\n",
    "        gray_img = imageio.imread(path_edged)\r\n",
    "        \r\n",
    "        hough_radii = np.arange(20, 80) # pupila por defecto\r\n",
    "        if borde == \"iris\":\r\n",
    "            hough_radii = np.arange(90, 160) # rango del iris\r\n",
    "        \r\n",
    "        hough_res = hough_circle(gray_img, hough_radii)\r\n",
    "        accums, cx, cy, radii = hough_circle_peaks(hough_res, hough_radii, total_num_peaks=1) \r\n",
    "\r\n",
    "        \r\n",
    "        return [cx[0], cy[0], radii[0]]\r\n",
    "\r\n",
    "    dict['general']['dataset_unif_segv2_edg_norm_dir'] = dict['general']['dataset_unif_segv2_edg_dir'] + \"_norm\"\r\n",
    "    \r\n",
    "    # os.mkdir(dict['general']['dataset_unif_segv2_edg_norm_dir'])\r\n",
    "\r\n",
    "    for file in os.listdir(dict['general']['dataset_unif_segv2_dir']):\r\n",
    "        \r\n",
    "        try:\r\n",
    "            path_sample = \"CASIA_IrisV1_unif\" + os.sep + file\r\n",
    "            sample_image = imageio.imread(path_sample)\r\n",
    "            boundaries,centers = [],[]\r\n",
    "            pupil_coord = get_circles(\"pupil\",file)\r\n",
    "            iris_coord = get_circles(\"iris\",file)\r\n",
    "            cx, cy, radius = list(zip(pupil_coord, iris_coord))\r\n",
    "            draw = draw_circles(sample_image,cx,cy,radius)\r\n",
    "\r\n",
    "            boundaries.append(draw)\r\n",
    "            data = {'pupil':pupil_coord,\r\n",
    "                    'iris':iris_coord}\r\n",
    "            centers.append(data)\r\n",
    "\r\n",
    "            print(centers)\r\n",
    "\r\n",
    "            target = [img for img in boundaries]\r\n",
    "            normalized=[]\r\n",
    "            cent=0\r\n",
    "            \r\n",
    "            for img in target:\r\n",
    "                #load pupil centers and radius of inner circles\r\n",
    "                center_x = centers[cent][0]\r\n",
    "                center_y = centers[cent][1]\r\n",
    "                radius_pupil=int(centers[cent][2])\r\n",
    "\r\n",
    "                iris_radius = 53 # width of space between inner and outer boundary\r\n",
    "\r\n",
    "                #define equally spaced interval to iterate over\r\n",
    "                nsamples = 360\r\n",
    "                samples = np.linspace(0,2*np.pi, nsamples)[:-1]\r\n",
    "                polar = np.zeros((iris_radius, nsamples))\r\n",
    "                for r in range(iris_radius):\r\n",
    "                    for theta in samples:\r\n",
    "                        #get x and y for values on inner boundary\r\n",
    "                        x = (r+radius_pupil)*np.cos(theta)+center_x\r\n",
    "                        y = (r+radius_pupil)*np.sin(theta)+center_y\r\n",
    "                        x=int(x)\r\n",
    "                        y=int(y)\r\n",
    "                        try:\r\n",
    "                        #convert coordinates\r\n",
    "                            polar[r][int((theta*nsamples)/(2*np.pi))] = img[y][x]\r\n",
    "                        except IndexError: #ignores values which lie out of bounds\r\n",
    "                            pass\r\n",
    "                        continue\r\n",
    "                res = cv2.resize(polar,(512,64))\r\n",
    "                normalized.append(res)\r\n",
    "                cent+=1\r\n",
    "\r\n",
    "            h,w = normalized[0].shape # height, width\r\n",
    "\r\n",
    "            roi = normalized[0][10:h, 0:int(512/2)]\r\n",
    "            roi = roi.astype(np.uint8) \r\n",
    "            roi_enhanced = cv2.equalizeHist(roi)\r\n",
    "            # cv2.imwrite(dict['general']['dataset_unif_segv2_edg_norm_dir'] + os.sep + file, roi_enhanced)\r\n",
    "        except:\r\n",
    "            print(\"->Ha ocurrido un error durante la etapa de normalización\")\r\n",
    "\r\n",
    "\r\n",
    "    # cogemos nombres de las imágenes .bmp\r\n",
    "    image_files = []\r\n",
    "    new_dir = 'CASIA_IrisV1_unif_segv2_edg_norm'\r\n",
    "\r\n",
    "    for root, dirs, files in os.walk(new_dir):\r\n",
    "        for file in files:\r\n",
    "            if file.endswith(\".bmp\"):\r\n",
    "                image_files.append(file)\r\n",
    "                \r\n",
    "    # cogemos las clases a las que pertenece cada .bmp\r\n",
    "    classes = []           \r\n",
    "    for name in image_files:\r\n",
    "        classes.append(name.split('_', 1)[0])\r\n",
    "     \r\n",
    "    x_pupil, y_pupil, r_pupil =[], [], []\r\n",
    "    x_iris, y_iris, r_iris =[],[],[]\r\n",
    "\r\n",
    "    print(centers)\r\n",
    "\r\n",
    "    for i in centers:\r\n",
    "        x_pupil.append(i['pupil'][0])\r\n",
    "        y_pupil.append(i['pupil'][1])\r\n",
    "        r_pupil.append(i['pupil'][2])\r\n",
    "        x_iris.append(i['iris'][0])\r\n",
    "        y_iris.append(i['iris'][1])\r\n",
    "        r_iris.append(i['iris'][2])\r\n",
    "    \r\n",
    "\r\n",
    "    data = {'image': image_files,\r\n",
    "       'pupil x_center':x_pupil,\r\n",
    "       'pupil y_center':y_pupil,\r\n",
    "       'pupil radius': r_pupil,\r\n",
    "       'iris x_center': x_iris,\r\n",
    "       'iris y_center': y_iris,\r\n",
    "       'iris radius':r_iris,\r\n",
    "       'class': classes}\r\n",
    "    \r\n",
    "    ################################# arrays must be the same lenght\r\n",
    "    df = pd.DataFrame(data)[['image','pupil x_center','pupil y_center','pupil radius','iris x_center', 'iris y_center','iris radius',\"class\"]] \r\n",
    "\r\n",
    "    df.to_csv(\"iris_data.csv\")\r\n",
    "\r\n",
    "    return dict\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## extraction()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extraction():\r\n",
    "    \r\n",
    "    print(\"Función 3, extraction()\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## clasification()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definir y ejecutar el pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definición del pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "try:\r\n",
    "    from sklearn.preprocessing import FunctionTransformer\r\n",
    "    from sklearn.pipeline import Pipeline\r\n",
    "except ImportError as e:\r\n",
    "    !pip install sklearn\r\n",
    "    from sklearn.preprocessing import FunctionTransformer\r\n",
    "    from sklearn.pipeline import Pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "_1_tratar_dataset_pip = FunctionTransformer(tratar_dataset_casia)\r\n",
    "_1_1_data_augmentation_pip = FunctionTransformer(data_augmentation)\r\n",
    "_2_1_segmentation_pip = FunctionTransformer(segmentation)\r\n",
    "_2_2_normalization_pip = FunctionTransformer(normalization)\r\n",
    "\r\n",
    "iris_recognition_pipeline = Pipeline([('_1_tratarDataset', _1_tratar_dataset_pip), \r\n",
    "                                    ('_1_1_dataAugmentation', _1_1_data_augmentation_pip), \r\n",
    "                                    ('_2_1_segmentation', _2_1_segmentation_pip), \r\n",
    "                                    ('_2_2_mormalization', _2_2_normalization_pip)])\r\n",
    "\r\n",
    "# iris_recognition_pipeline = Pipeline([('_1_tratarDataset', _1_tratar_dataset_pip), \r\n",
    "#                                     ('_2_1_segmentation', _2_1_segmentation_pip), \r\n",
    "#                                     ('_2_2_mormalization', _2_2_normalization_pip)])\r\n",
    "\r\n",
    "# iris_recognition_pipeline = Pipeline([('_2_2_mormalization', _2_2_normalization_pip)])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejecición pipeline\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "iris_recognition_pipeline.transform(confi_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Función 1, tratar_dataset_casia()\n",
      "-> Muestras previamente copiadas.\n",
      "Función 1.1, data_augmentation()\n",
      "->Saltando el ruido gausseano en data augmentation\n",
      "Función 2.1, segmentation()\n",
      "->Segmentaciones previamente guardadas\n",
      "->La segmentación v2 ya había sido creada\n",
      "->Segmentaciones V2 guardadas\n",
      "->Edge guardadas\n",
      "Función 2.2, normalization()\n",
      "[{'pupil': [182, 134, 33], 'iris': [176, 135, 105]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [173, 138, 35], 'iris': [169, 140, 104]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [174, 120, 35], 'iris': [168, 121, 107]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [183, 122, 37], 'iris': [179, 122, 104]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [177, 145, 35], 'iris': [173, 149, 109]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [179, 133, 36], 'iris': [177, 136, 108]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [154, 131, 33], 'iris': [157, 137, 100]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [181, 141, 47], 'iris': [158, 138, 93]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [184, 139, 45], 'iris': [186, 134, 118]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [172, 155, 44], 'iris': [145, 159, 90]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [194, 134, 47], 'iris': [197, 137, 114]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [157, 138, 47], 'iris': [130, 138, 92]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [168, 137, 48], 'iris': [172, 133, 118]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [172, 132, 48], 'iris': [176, 126, 118]}]\n",
      "->Ha ocurrido un error durante la etapa de normalización\n",
      "[{'pupil': [172, 132, 48], 'iris': [176, 126, 118]}]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13844/3981601267.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0miris_recognition_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfi_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \"\"\"\n\u001b[0;32m    181\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkw_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_identity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkw_args\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkw_args\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__sklearn_is_fitted__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13844/2692257053.py\u001b[0m in \u001b[0;36mnormalization\u001b[1;34m(dict)\u001b[0m\n\u001b[0;32m    174\u001b[0m        \u001b[1;34m'iris radius'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr_iris\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m        'class': classes}\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'pupil x_center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'pupil y_center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'pupil radius'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'iris x_center'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'iris y_center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'iris radius'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iris_datav.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit (system)"
  },
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}