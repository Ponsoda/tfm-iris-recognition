{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "confi_dict = {\r\n",
    "\r\n",
    "    'general':{\r\n",
    "        'root_dir':r\"C:\\Users\\na-ch\\Desktop\\estudio\\Master_Big_Data\\03_TFM\\02_Code\\dataset_test\",\r\n",
    "        'dataset_dir': \"CASIA_IrisV1\"\r\n",
    "    },\r\n",
    "\r\n",
    "    '1_tratarDataset': {\r\n",
    "        'show_first' : False\r\n",
    "    },\r\n",
    "\r\n",
    "    '1.1_dataAugmentation':{\r\n",
    "        'gaussianNoise' : False,\r\n",
    "        'stdGN': [5, 10, 15, 20, 25, 30],\r\n",
    "        'skipFiles': 1, \r\n",
    "        ''' \r\n",
    "        1 means that you are skiping 1 for the noise augmentation, 2 skiping 2 and so on. If you don't want to skip files set it as 0\r\n",
    "        '''\r\n",
    "        'afinTransformation': True\r\n",
    "    },\r\n",
    "\r\n",
    "    '2.1_segmentation':{\r\n",
    "        'redNeuronal' : \"Iris_unet_d5.h5\"\r\n",
    "    }\r\n",
    "\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1: Tratar el dataset CASIA\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tratar_dataset_casia()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def tratar_dataset_casia(dict):\r\n",
    "    \r\n",
    "    print(\"Función 1, tratar_dataset_casia()\")\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    try:\r\n",
    "        import shutil\r\n",
    "    except:\r\n",
    "        !pip install shutil\r\n",
    "        import shutil\r\n",
    "    try:\r\n",
    "         import imageio\r\n",
    "    except:\r\n",
    "        !pip install imageio\r\n",
    "        import imageio\r\n",
    "    try:\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    except:\r\n",
    "        !pip install matplotlib\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "    \r\n",
    "    def copy_all_samples(path_samples, destination_directory):\r\n",
    "        '''\r\n",
    "        Copia todas las muestras del dataset de CASIA a un único directorio.\r\n",
    "        '''\r\n",
    "        if not os.path.exists(path_samples):\r\n",
    "            print(\"El directorio indicado como input no existe\")\r\n",
    "        elif not os.path.exists(destination_directory):\r\n",
    "            ''' \r\n",
    "            Comprobamos que el directorio del output no existe\r\n",
    "            '''\r\n",
    "            os.mkdir(destination_directory) # Creamos un nuevo directorio \r\n",
    "            for root, dirs, files in os.walk(path_samples):\r\n",
    "                for file in files:\r\n",
    "                    path_file = os.path.join(root,file)\r\n",
    "                    shutil.copy2(path_file,destination_directory)   \r\n",
    "            print(\"-> Muestras copiadas satisfactoriamente.\")\r\n",
    "        elif len(os.listdir(destination_directory)) < 1:\r\n",
    "            ''' \r\n",
    "            Comprobamos que el directorio del output no está vacío\r\n",
    "            '''\r\n",
    "            for root, dirs, files in os.walk(path_samples):\r\n",
    "                for file in files:\r\n",
    "                    path_file = os.path.join(root,file)\r\n",
    "                    shutil.copy2(path_file,destination_directory)   \r\n",
    "            print(\"-> Muestras copiadas satisfactoriamente.\")\r\n",
    "        else:\r\n",
    "            print(\"-> Muetras previamente copiadas.\")\r\n",
    "\r\n",
    "    copy_all_samples(dict['general']['dataset_dir'], dict['general']['dataset_dir']+\"_Unificado\")\r\n",
    "\r\n",
    "    if dict['1_tratarDataset']['show_first']:\r\n",
    "        ''' \r\n",
    "        Si en la configuración show_first es True, se mostrará el primer elemento del nuevo directorio.\r\n",
    "        '''\r\n",
    "        path_img = dict['general']['dataset_dir']+\"_Unificado\" + os.sep + os.listdir(dict['general']['dataset_dir']+\"_Unificado\")[0]\r\n",
    "        img = imageio.imread(path_img)\r\n",
    "        plt.title(\"Primer elemento de \" + dict['general']['dataset_dir']+\"_Unificado\")\r\n",
    "        plt.imshow(img)\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Pasando el diccionario con la configuración a la siguiente función\r\n",
    "    '''\r\n",
    "    return dict\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1: Data Augmentation\r\n",
    "data_augmentation()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "def data_augmentation(dict):\r\n",
    "\r\n",
    "    print(\"Función 1.1, data_augmentation()\")\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    try:\r\n",
    "        import imageio\r\n",
    "    except:\r\n",
    "        !pip install imageio\r\n",
    "        import imageio\r\n",
    "    try:\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    except:\r\n",
    "        !pip install matplotlib\r\n",
    "        import matplotlib.pyplot as plt\r\n",
    "    try:\r\n",
    "        from tensorflow.keras.layers import GaussianNoise\r\n",
    "    except:\r\n",
    "        !pip install tensorflow\r\n",
    "        from tensorflow.keras.layers import GaussianNoise\r\n",
    "    try:\r\n",
    "        import numpy as np\r\n",
    "    except:\r\n",
    "        !pip install numpy\r\n",
    "        import numpy as np\r\n",
    "    import random\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "    \r\n",
    "    ''' \r\n",
    "    Comprobamos si el ruido gaussiano ha sido establecido como True en la configuración\r\n",
    "    '''\r\n",
    "\r\n",
    "    if dict['1.1_dataAugmentation']['gaussianNoise']:\r\n",
    "\r\n",
    "        ''' \r\n",
    "        Comprobamos si el ruido gaussiano se ha aplicado anteriormente, si es así, el primer elemento del directorio debería de tener el sufijo _augmentation\r\n",
    "        '''\r\n",
    "        if \"_augment\" in os.listdir(dict['general']['dataset_dir'] + \"_Unificado\")[1]:\r\n",
    "            print(\"El ruido gaussiano ya se ha ejecutado con anterioridad\")\r\n",
    "        else:     \r\n",
    "            ''' \r\n",
    "            Desde el directorio unificado creamos nuevas imágenes utilizando el ruido gaussiano como data augmentation.\r\n",
    "            Añadimos un contador para que el augmentation sea de 1 cada dos imágenes.\r\n",
    "            '''\r\n",
    "            i = 0\r\n",
    "            for filename in os.listdir(dict['general']['dataset_dir'] + \"_Unificado\"):\r\n",
    "                if dict['1.1_dataAugmentation']['skipFiles'] == 0:\r\n",
    "                    path = dict['general']['dataset_dir'] + \"_Unificado\" + os.sep + filename\r\n",
    "                    image = imageio.imread(path)/255\r\n",
    "                    std_rdm = random.choice(dict['1.1_dataAugmentation']['stdGN'])\r\n",
    "                    gaussean_function = GaussianNoise(std_rdm/100)\r\n",
    "                    noisey = gaussean_function(image.astype(np.float32),training=True)\r\n",
    "                    name = path[:-4] + '_augment.bmp'\r\n",
    "                    plt.imsave(name, noisey, cmap='gray')\r\n",
    "                elif i < dict['1.1_dataAugmentation']['skipFiles']:\r\n",
    "                    path = dict['general']['dataset_dir'] + \"_Unificado\" + os.sep + filename\r\n",
    "                    image = imageio.imread(path)/255\r\n",
    "                    std_rdm = random.choice(dict['1.1_dataAugmentation']['stdGN'])\r\n",
    "                    gaussean_function = GaussianNoise(std_rdm/100)\r\n",
    "                    noisey = gaussean_function(image.astype(np.float32),training=True)\r\n",
    "                    name = path[:-4] + '_augment.bmp'\r\n",
    "                    plt.imsave(name, noisey, cmap='gray')\r\n",
    "                    i += 1\r\n",
    "                else:\r\n",
    "                    i = 0\r\n",
    "    else:\r\n",
    "        print(\"Saltando el ruido gausseano en data augmentation\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Pasando el diccionario con la configuración a la siguiente función\r\n",
    "    '''\r\n",
    "    return dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Red U-Net v2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 segmentation()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def segmentation(dict):\r\n",
    "\r\n",
    "    print(\"Función 2.1, segmentation()\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalization(dict):\r\n",
    "\r\n",
    "    print(\"Función 2.2, normalization()\")\r\n",
    "\r\n",
    "    ''' \r\n",
    "    Importando librerías\r\n",
    "    '''\r\n",
    "    try:\r\n",
    "        import os\r\n",
    "    except:\r\n",
    "        !pip install os\r\n",
    "        import os\r\n",
    "    ''' \r\n",
    "    Estableciendo la root de la función\r\n",
    "    '''\r\n",
    "    os.chdir(dict['general']['root_dir'])\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definir y ejecutar el pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definición del pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "try:\r\n",
    "    from sklearn.preprocessing import FunctionTransformer\r\n",
    "    from sklearn.pipeline import Pipeline\r\n",
    "except ImportError as e:\r\n",
    "    !pip install sklearn\r\n",
    "    from sklearn.preprocessing import FunctionTransformer\r\n",
    "    from sklearn.pipeline import Pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "_1_tratar_dataset_pip = FunctionTransformer(tratar_dataset_casia)\r\n",
    "_1_1_data_augmentation_pip = FunctionTransformer(data_augmentation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "iris_recognition_pipeline = Pipeline([('_1_tratarDataset', _1_tratar_dataset_pip), ('_1_1_dataAugmentation', _1_1_data_augmentation_pip)])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejecición pipeline\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "iris_recognition_pipeline.transform(confi_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Función 1, tratar_dataset_casia()\n",
      "-> Muetras previamente copiadas.\n",
      "Función 1.1, data_augmentation()\n",
      "Saltando el ruido gausseano en data augmentation\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'general': {'root_dir': 'C:\\\\Users\\\\na-ch\\\\Desktop\\\\estudio\\\\Master_Big_Data\\\\03_TFM\\\\02_Code\\\\dataset_test',\n",
       "  'dataset_dir': 'CASIA_IrisV1'},\n",
       " '1_tratarDataset': {'show_first': False},\n",
       " '1.1_dataAugmentation': {'gaussianNoise': False,\n",
       "  'stdGN': [5, 10, 15, 20, 25, 30],\n",
       "  'skipFiles': 1,\n",
       "  \" \\n        1 means that you are skiping 1 for the noise augmentation, 2 skiping 2 and so on. If you don't want to skip files set it as 0\\n        afinTransformation\": True}}"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit"
  },
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}